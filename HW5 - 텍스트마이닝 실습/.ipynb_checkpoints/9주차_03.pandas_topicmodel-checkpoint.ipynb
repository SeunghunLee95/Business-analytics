{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰 수: 14967\n",
      "리뷰영화: {'인피니티 워', '신과함께', '곤지암', '코코', '라라랜드', '범죄도시', '택시운전사'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "movie = pd.read_csv('movie_data_new.csv', header = None) # header를 None으로 설정해 header가 없음을 알림\n",
    "\n",
    "print('리뷰 수:', len(movie))\n",
    "print('리뷰영화:', set(movie.iloc[:,2])) # ix가 사라져 iloc으로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>헐..다 죽었어....나중에 앤트맨 보다가도 깜놀...</td>\n",
       "      <td>10</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>충격 결말</td>\n",
       "      <td>9</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>응집력</td>\n",
       "      <td>8</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>개연성은 무시해라 액션을 즐겨라 스타로드가 이끌어준다 각각의 영웅들을 즐겨라 그리고...</td>\n",
       "      <td>8</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>내가졸라이상하네</td>\n",
       "      <td>4</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>대박</td>\n",
       "      <td>10</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>정말 지루할틈없이 넘잘만들었다 역시 대단하다</td>\n",
       "      <td>9</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>역시 어벤져스!!</td>\n",
       "      <td>9</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>마지막에 누구한테 연락한거지? 궁금</td>\n",
       "      <td>9</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>다음 편이 궁굼해지네요^^</td>\n",
       "      <td>10</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0   1       2\n",
       "0                     헐..다 죽었어....나중에 앤트맨 보다가도 깜놀...  10  인피니티 워\n",
       "1                                              충격 결말   9  인피니티 워\n",
       "2                                                응집력   8  인피니티 워\n",
       "3  개연성은 무시해라 액션을 즐겨라 스타로드가 이끌어준다 각각의 영웅들을 즐겨라 그리고...   8  인피니티 워\n",
       "4                                           내가졸라이상하네   4  인피니티 워\n",
       "5                                                 대박  10  인피니티 워\n",
       "6                           정말 지루할틈없이 넘잘만들었다 역시 대단하다   9  인피니티 워\n",
       "7                                          역시 어벤져스!!   9  인피니티 워\n",
       "8                                마지막에 누구한테 연락한거지? 궁금   9  인피니티 워\n",
       "9                                     다음 편이 궁굼해지네요^^  10  인피니티 워"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = movie.iloc[:,0] # 전체행에 대해 1번째 열만 인덱싱 - 리뷰들만 가져옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### topic modeling은 count vector를 사용하므로 sklearn의 CountVectorizer를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt #konlpy에서 Twitter 형태소 분석기를 import\n",
    "twitter_tag = Okt()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(tokenizer=twitter_tag.nouns, #우선은 명사만 사용\n",
    "                      decode_error ='ignore', \n",
    "                      max_df=0.5, #너무 자주 나타나는 단어는 제외, 책에서는 0.15를 사용\n",
    "                      #min_df = 3, #3개 미만의 문서에서 나타난 단어는 제외, 여기서는 max_features를 1000으로 제한하므로 별 필요 없음\n",
    "                      max_features = 1000) #적당한 대상 단어 수를 선택\n",
    "review_cv = vec.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14967, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_cv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn의 LatentDirichletAllocation을 이용하여 topic modeling 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = 10, #추출할 topic의 수를 지정\n",
    "                                max_iter=25, #max_iter는 최종 모형에서는 5,000~10,000번 시도한다고 알려져 있음\n",
    "                                topic_word_prior= 0.1, doc_topic_prior=1.0,\n",
    "                                #topic_word_prior: beta, doc_topic_prior: alpha\n",
    "                                #일반적으로 beta는 0.1로 고정하고 alpha를 50/topic의 수 근처의 값을 시도\n",
    "                                #alpha와 beta는 낮은 값을 사용할수록 특정 토픽들이 두드러지도록 하는 효과가 있다고 합\n",
    "                                learning_method='batch', #'batch'는 'online'에 비해 더 성능이 좋고 느림, 현재는 default\n",
    "                                n_jobs= -1, #사용 processor 수, None이면 1, -1이면 모두 사용\n",
    "                                random_state=0)\n",
    "\n",
    "review_topics = lda.fit_transform(review_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic-word distribution dimension: (10, 1000)\n",
      "document-topic distribution dimenstion (14967, 10)\n"
     ]
    }
   ],
   "source": [
    "print('topic-word distribution dimension:', lda.components_.shape)\n",
    "print('document-topic distribution dimenstion', review_topics.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### topic을 보기 쉽게 출력하는 함수를 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d: \" % topic_idx, end='')\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        #print(\", \".join([feature_names[i]+'('+str(topic[i])+')' for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        # 위 slicing에서 맨 뒤 -1은 역순을 의미, 역순으로 했을 때 처음부터 n_top_words까지\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 정말, 생각, 기대, 원작, 내, 안, 인생, 작품, 난, 굿\n",
      "Topic #1: 더, 그, 광주, 말, 다시, 가슴, 우리, 한번, 송강호, 택시\n",
      "Topic #2: 것, 좀, 장면, 때, 나, 음악, 사랑, 이야기, 번, 현실\n",
      "Topic #3: 스토리, 시간, 돈, 정도, 신파, 감독, 거, 한국, 듯, 수준\n",
      "Topic #4: 그냥, 편, 마블, 뭐, 이해, 공포, 역시, 노스, 느낌, 다음\n",
      "Topic #5: 마동석, 최고, 볼, 함, 액션, 공포영화, 임, 완전, 걸, 중\n",
      "Topic #6: 사람, 점, 수, 꼭, 부분, 이, 분, 웹툰, 조금, 인간\n",
      "Topic #7: 감동, 눈물, 재미, 마지막, 보고, 내용, 가족, 처음, 끝, 추천\n",
      "Topic #8: 연기, 배우, 진짜, 왜, 윤계상, 차태현, 연출, 또, 대박, 모두\n",
      "Topic #9: 영화, 이, 평점, 만, 관객, 신, 천만, 비, 알바, 제\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda,vec.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한 자로 구성된 명사들 삭제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['충격', '결말']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_tag.nouns(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(str):\n",
    "    return [token for token in twitter_tag.nouns(str) if len(token) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['충격', '결말']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count vector 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vector 새로 생성\n",
    "vec = CountVectorizer(tokenizer=tokenizer, decode_error ='ignore', \n",
    "                      max_df=0.5, max_features = 1000) #너무 자주 나타나는 단어는 제외\n",
    "review_cv = vec.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic modeling 수행\n",
    "lda = LatentDirichletAllocation(n_components = 50, max_iter=25, \n",
    "                                topic_word_prior= 0.1, doc_topic_prior=1.0,\n",
    "                                learning_method='batch',\n",
    "                                n_jobs= -1,\n",
    "                                random_state=0)\n",
    "review_topics = lda.fit_transform(review_cv)\n",
    "print_top_words(lda,vec.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  topic들을 대상으로 clustering을 수행해서 topic들이 잘 묶을 수 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5) #5개 클러스터로 분류\n",
    "kmeans.fit(lda.components_)\n",
    "\n",
    "print('topic-word distribution dimension:', lda.components_.shape)\n",
    "print('Cluster label:', kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-SNE를 이용하여 각 topic들 간의 관계를 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_review = tsne.fit_transform(lda.components_)\n",
    "print('TSNE dimension:', tsne_review.shape)\n",
    "tsne_review[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "import matplotlib as mpl\n",
    "\n",
    "# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {0:'blue', 1:'yellow', 2:'red', 3:'green', 4:'purple'}\n",
    "x1 = tsne_review[:,0]\n",
    "x2 = tsne_review[:,1]\n",
    "plt.scatter(x1, x2)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.xlim(min(x1), max(x1))\n",
    "plt.ylim(min(x2), max(x2))\n",
    "for i in range(len(x1)):\n",
    "#    plt.text(X1_lsi[i], X2_lsi[i], sample_label[i],\n",
    "#            color = colors[sample_label[i]])\n",
    "    plt.text(x1[i], x2[i], kmeans.labels_[i],\n",
    "            color = colors[kmeans.labels_[i]])\n",
    "\n",
    "#for word, pos in df.iterrows():\n",
    "#    ax.annotate(word, pos, fontsize=30)\n",
    "plt.show()\n",
    "\n",
    "#아래 결과를 보면 각 topic들이 고르게 분포하고 있어, 클러스터링이 별로 의미 없음을 알 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최적의 결과를 찾기 위한 방법\n",
    "\n",
    "+ 적절한 topic의 수는 perplexity 값을 이용해 찾음.\n",
    "+ 낮을 수록 좋은 모형이나, 사람의 판단과 일치하지 않을 수 있음\n",
    "+ alpha, beta는 앞서 설명한 바와 같이 beta=0.1로 고정하고 alpha를 50/토픽수 근처에서 찾아나감\n",
    "+ 반복횟수는 5,000번 정도면 수렴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.perplexity(review_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic modeling 수행\n",
    "lda = LatentDirichletAllocation(n_components = 25, max_iter=25, \n",
    "                                topic_word_prior= 0.1, doc_topic_prior=2.0,\n",
    "                                learning_method='batch',\n",
    "                                n_jobs= -1,\n",
    "                                random_state=0)\n",
    "review_topics = lda.fit_transform(review_cv)\n",
    "print_top_words(lda,vec.get_feature_names(), 10)\n",
    "lda.perplexity(review_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic modeling 수행\n",
    "lda = LatentDirichletAllocation(n_components = 10, max_iter=25, \n",
    "                                topic_word_prior= 0.1, doc_topic_prior=1.0,\n",
    "                                learning_method='batch',\n",
    "                                n_jobs= -1,\n",
    "                                random_state=0)\n",
    "review_topics = lda.fit_transform(review_cv)\n",
    "print_top_words(lda,vec.get_feature_names(), 10)\n",
    "lda.perplexity(review_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
