{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  웹 크롤링1 - Static Crawling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request # urllib는 url을 다루는 모듈 모아 놓은 패키지 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. urllib.request를 이용한 다운로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 읽어들이기 \n",
    "from urllib import request\n",
    "\n",
    "url=\"http://uta.pw/shodou/img/28/214.png\" # url 읽어오기 \n",
    "savename=\"test.png\"  # test.png 만든다.\n",
    "\n",
    "request.urlretrieve(url,savename) # png 파일을 test.png로 저장\n",
    "print('저장되었습니다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. urlopen으로 파일에 저장하는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다..\n"
     ]
    }
   ],
   "source": [
    "# URL과 저장경로 지정하기\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"test1.png\"\n",
    "#다운로드\n",
    "mem = request.urlopen(url).read() # 메모리에 데이터를 올린 후 파일에 저장 \n",
    "#파일로 저장하기, wb는 쓰기와 바이너리모드\n",
    "with open(savename, mode=\"wb\") as f: \n",
    "    f.write(mem)\n",
    "    print(\"저장되었습니다..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. API 사용하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ip]\n",
      "API_URI=http://api.aoikujira.com/ip/get.php\n",
      "REMOTE_ADDR=14.39.246.6\n",
      "REMOTE_HOST=14.39.246.6\n",
      "REMOTE_PORT=50698\n",
      "HTTP_HOST=api.aoikujira.com\n",
      "HTTP_USER_AGENT=Python-urllib/3.7\n",
      "HTTP_ACCEPT_LANGUAGE=\n",
      "HTTP_ACCEPT_CHARSET=\n",
      "SERVER_PORT=80\n",
      "FORMAT=ini\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 읽어들이기 \n",
    "url=\"http://api.aoikujira.com/ip/ini\" # url을 가져온다\n",
    "res=request.urlopen(url) # 메모리에 데이터를 올린 후 파일에 저장 \n",
    "data=res.read() # 읽어들임\n",
    "\n",
    "#바이너리를 문자열로 변환하기\n",
    "text=data.decode(\"utf-8\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패키지 import 및 예제 HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # HTML/XML에서 정보를 추출할 수 있도록 도와주는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <h1>스크레이핑이란?</h1>\n",
    "  <p>웹 페이지를 분석하는 것</p>\n",
    "  <p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. 기본 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2) HTML 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser') # html 가져오기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 원하는 부분 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = soup.html.body.h1 # h1 태그를 접근 \n",
    "p1 = soup.html.body.p  # p 태그는 2개 두개로 나눠서 접근 \n",
    "p2 = p1.next_sibling.next_sibling # next_sibling을 두번 이용하여 P추출 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 요소의 글자 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 스크레이핑이란?\n",
      "p  = 웹 페이지를 분석하는 것\n",
      "p  = 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "print(f\"h1 = {h1.string}\") # 위에서 추출한 부분에 요소 출력 \n",
    "print(f\"p  = {p1.string}\")\n",
    "print(f\"p  = {p2.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. 요소를 찾는 method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단일 element 추출: find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser') # html 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 1) find() 메서드로 원하는 부분 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>스크레이핑이란?</h1>\n"
     ]
    }
   ],
   "source": [
    "title = soup.find(\"h1\") # find()를 통해 원하는 부분 추출 \n",
    "body  = soup.find(\"p\")\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 2) 텍스트 부분 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#title = 스크레이핑이란?\n",
      "#body = 웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "print(f\"#title = {title.string}\" )  # 텍스트 부분 출력 \n",
    "print(f\"#body = {body.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 복수 elements 추출: find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러개의 태그를 추출 \n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <ul>\n",
    "    <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "    <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "  </ul>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 1) find_all() 메서드로 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://www.naver.com\">naver</a>, <a href=\"http://www.daum.net\">daum</a>] 2\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all(\"a\") # 복수의 요소를 추출해서 \n",
    "print(links, len(links))   # 링크와 길이 출력    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 2) 링크 목록 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver > http://www.naver.com\n",
      "daum > http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "for a in links:\n",
    "    href = a.attrs['href'] # href의 속성에 있는 속성값을 추출\n",
    "    text = a.string \n",
    "    print(text, \">\", href) # 텍스트인 naver,daum과 href의 속성에 있는 속성값 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Css Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup에서 Css Selector 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html><body>\n",
    "<div id=\"meigen\">\n",
    "  <h1>위키북스 도서</h1>\n",
    "  <ul class=\"items\">\n",
    "    <li>유니티 게임 이펙트 입문</li>\n",
    "    <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "    <li>모던 웹사이트 디자인의 정석</li>\n",
    "  </ul>\n",
    "</div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기 \n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 필요한 부분을 CSS 쿼리로 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 위키북스 도서\n",
      "li = 유니티 게임 이펙트 입문\n",
      "li = 스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "li = 모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "# 타이틀 부분 추출하기 --- (※3)\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string # CSS 선택자로 요소 하나를 추출 \n",
    "print(f\"h1 = {h1}\")\n",
    "\n",
    "# 목록 부분 추출하기 --- (※4)\n",
    "li_list = soup.select(\"div#meigen > ul.items > li\") # CSS 선택자로 요소 여러 개를 리스트로 추출\n",
    "for li in li_list:\n",
    "  print(f\"li = {li.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 활용 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. 네이버 금융 - 환율 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) HTML 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://finance.naver.com/marketindex/\" # 네이버 금융 url 불러오기 \n",
    "res = request.urlopen(url) # 메모리에 데이터를 올린 후 파일에 저장 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)HTML 분석하기¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 원하는 데이터 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/krw = 1,175.00\n"
     ]
    }
   ],
   "source": [
    "price = soup.select_one(\"div.head_info > span.value\").string # 요소 하나 \n",
    "print(\"usd/krw =\", price) # 가격 출력 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 기상청 RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) HTML 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url= http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "#매개변수를 URL로 인코딩한다.\n",
    "values = {\n",
    "    'stnId':'109'\n",
    "}\n",
    "\n",
    "params=parse.urlencode(values)\n",
    "url += \"?\"+params # URL에 매개변수 추가\n",
    "print(\"url=\", url)\n",
    "\n",
    "res = request.urlopen(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) HTML 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 원하는 데이터 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울,경기도 육상중기예보\n",
      "○ (강수) 1일(목) 오후~2일(금) 오전에는 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 낮 기온은 20~25도로 오늘(25일, 24~27도)과 비슷하거나 조금 낮겠고, 아침 기온은 9~17도로 선선하겠습니다.<br />          특히, 내륙을 중심으로 낮과 밤의 기온차가 10도 내외로 크겠습니다.<br />○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다.\n"
     ]
    }
   ],
   "source": [
    "header = soup.find(\"header\") # header를 찾아 header에 저장 \n",
    "\n",
    "title = header.find(\"title\").text  # header 요소 중 title\n",
    "wf = header.find(\"wf\").text        # header 요소 중 wf\n",
    "\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ css selector 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울,경기도 육상중기예보\n",
      "○ (강수) 1일(목) 오후~2일(금) 오전에는 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 낮 기온은 20~25도로 오늘(25일, 24~27도)과 비슷하거나 조금 낮겠고, 아침 기온은 9~17도로 선선하겠습니다.<br />          특히, 내륙을 중심으로 낮과 밤의 기온차가 10도 내외로 크겠습니다.<br />○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다.\n"
     ]
    }
   ],
   "source": [
    "title = soup.select_one(\"header > title\").text\n",
    "wf = header.select_one(\"header wf\").text\n",
    "\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. 윤동주 작가의 작품 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 하늘과 바람과 별과 시\n",
      "- 증보판\n",
      "- 서시\n",
      "- 자화상\n",
      "- 소년\n",
      "- 눈 오는 지도\n",
      "- 돌아와 보는 밤\n",
      "- 병원\n",
      "- 새로운 길\n",
      "- 간판 없는 거리\n",
      "- 태초의 아침\n",
      "- 또 태초의 아침\n",
      "- 새벽이 올 때까지\n",
      "- 무서운 시간\n",
      "- 십자가\n",
      "- 바람이 불어\n",
      "- 슬픈 족속\n",
      "- 눈감고 간다\n",
      "- 또 다른 고향\n",
      "- 길\n",
      "- 별 헤는 밤\n",
      "- 흰 그림자\n",
      "- 사랑스런 추억\n",
      "- 흐르는 거리\n",
      "- 쉽게 씌어진 시\n",
      "- 봄\n",
      "- 참회록\n",
      "- 간(肝)\n",
      "- 위로\n",
      "- 팔복\n",
      "- 못자는밤\n",
      "- 달같이\n",
      "- 고추밭\n",
      "- 아우의 인상화\n",
      "- 사랑의 전당\n",
      "- 이적\n",
      "- 비오는 밤\n",
      "- 산골물\n",
      "- 유언\n",
      "- 창\n",
      "- 바다\n",
      "- 비로봉\n",
      "- 산협의 오후\n",
      "- 명상\n",
      "- 소낙비\n",
      "- 한난계\n",
      "- 풍경\n",
      "- 달밤\n",
      "- 장\n",
      "- 밤\n",
      "- 황혼이 바다가 되어\n",
      "- 아침\n",
      "- 빨래\n",
      "- 꿈은 깨어지고\n",
      "- 산림\n",
      "- 이런날\n",
      "- 산상\n",
      "- 양지쪽\n",
      "- 닭\n",
      "- 가슴 1\n",
      "- 가슴 2\n",
      "- 비둘기\n",
      "- 황혼\n",
      "- 남쪽 하늘\n",
      "- 창공\n",
      "- 거리에서\n",
      "- 삶과 죽음\n",
      "- 초한대\n",
      "- 산울림\n",
      "- 해바라기 얼굴\n",
      "- 귀뚜라미와 나와\n",
      "- 애기의 새벽\n",
      "- 햇빛·바람\n",
      "- 반디불\n",
      "- 둘 다\n",
      "- 거짓부리\n",
      "- 눈\n",
      "- 참새\n",
      "- 버선본\n",
      "- 편지\n",
      "- 봄\n",
      "- 무얼 먹구 사나\n",
      "- 굴뚝\n",
      "- 햇비\n",
      "- 빗자루\n",
      "- 기왓장 내외\n",
      "- 오줌싸개 지도\n",
      "- 병아리\n",
      "- 조개껍질\n",
      "- 겨울\n",
      "- 트루게네프의 언덕\n",
      "- 달을 쏘다\n",
      "- 별똥 떨어진 데\n",
      "- 화원에 꽃이 핀다\n",
      "- 종시\n"
     ]
    }
   ],
   "source": [
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\" # url 가져오기\n",
    "res = request.urlopen(url) # 메모리에 데이터를 올린 후 파일에 저장  \n",
    "soup = BeautifulSoup(res, \"html.parser\") # html 가져오기 \n",
    "\n",
    "a_list = soup.select(\"#mw-content-text   ul > li  a\") # css selector로 원하는 데이터 추출 \n",
    "for a in a_list:\n",
    "    name = a.string\n",
    "    print(f\"- {name}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 일반 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 네이버  뉴스 헤드라인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                        정의당 대표 선거 결선으로…‘김종철 vs 배진교’ 2파전\n",
      "                                    \n",
      "\n",
      "                                        ‘김정은 사과’ 내세워 돌변… 대북규탄결의안 발빼는 與\n",
      "                                    \n",
      "\n",
      "                                        최대집 의협 회장 불신임안 부결… 내년 4월 임기 채운다\n",
      "                                    \n",
      "\n",
      "                                        “대단히 미안”하다면서 또 NLL 걸고 넘어진 북한\n",
      "                                    \n",
      "\n",
      "                                        술 취해 차로 행인 치고 포장마차 손님까지 들이받아 12명 부상\n",
      "                                    \n"
     ]
    }
   ],
   "source": [
    "# 네이버에서 크롤링이 안되기 때문에 \n",
    "# headers 정보에  User-Agent 를 넣어줌\n",
    "# 헤더(header)는, 접속하는 사람/프로그램에 대한 정보를 가지고 있음\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
    "url =\"https://news.naver.com/\"\n",
    "res = requests.get(url, headers = headers).text # url을 요청해서 텍스트화해서 저장 \n",
    "\n",
    "soup = BeautifulSoup(res, \"html.parser\") # html 가져오기 \n",
    "\n",
    "selector = \"#today_main_news > div.hdline_news > ul > li > div.hdline_article_tit > a\"\n",
    "# 원하는 데이터 추출 \n",
    "\n",
    "for a in soup.select(selector): # selector 안의 요소 반복 \n",
    "    title = a.text # a에서 text를 title에 저장\n",
    "    print(title)   # title 출력 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 시민의 소리 게시판"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['관리인 마스크', '어린이 대공원 쓰레기집하장 내 쓰레기 제거 요청 ', '마스크미착용으로 축구 및, 베트민턴 치는 인원이 너무 많아요.', '공원 내 마스크 착용', '청춘핫도그 점장님과 직원분께 감사드립니다', '카드결제를 거부하는 매점을 신고합니다', '참얼굴만큼예쁘고맘씨좋은 여직원을 만나 고마워서 글을남깁니다.', '놀이동산에서 불쾌함을 겪었습니다', '서문 플래카드', '간만에 친절한 아가씨를 만났어요.(놀이동산)'] ['https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=k3asiGOo04zZgXj2ohi1HKFeXa4NEVxYoPYSHDV1Iypw4FHlVfYva2CunMoNXECl.etisw2_servlet_user?qnaid=QNAS20200917000010&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=k3asiGOo04zZgXj2ohi1HKFeXa4NEVxYoPYSHDV1Iypw4FHlVfYva2CunMoNXECl.etisw2_servlet_user?qnaid=QNAS20200902000003&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=k3asiGOo04zZgXj2ohi1HKFeXa4NEVxYoPYSHDV1Iypw4FHlVfYva2CunMoNXECl.etisw2_servlet_user?qnaid=QNAS20200826000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=k3asiGOo04zZgXj2ohi1HKFeXa4NEVxYoPYSHDV1Iypw4FHlVfYva2CunMoNXECl.etisw2_servlet_user?qnaid=QNAS20200825000003&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=k3asiGOo04zZgXj2ohi1HKFeXa4NEVxYoPYSHDV1Iypw4FHlVfYva2CunMoNXECl.etisw2_servlet_user?qnaid=QNAS20200818000009&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=k3asiGOo04zZgXj2ohi1HKFeXa4NEVxYoPYSHDV1Iypw4FHlVfYva2CunMoNXECl.etisw2_servlet_user?qnaid=QNAS20200816000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=k3asiGOo04zZgXj2ohi1HKFeXa4NEVxYoPYSHDV1Iypw4FHlVfYva2CunMoNXECl.etisw2_servlet_user?qnaid=QNAS20200813000003&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=k3asiGOo04zZgXj2ohi1HKFeXa4NEVxYoPYSHDV1Iypw4FHlVfYva2CunMoNXECl.etisw2_servlet_user?qnaid=QNAS20200813000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=k3asiGOo04zZgXj2ohi1HKFeXa4NEVxYoPYSHDV1Iypw4FHlVfYva2CunMoNXECl.etisw2_servlet_user?qnaid=QNAS20200730000004&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=k3asiGOo04zZgXj2ohi1HKFeXa4NEVxYoPYSHDV1Iypw4FHlVfYva2CunMoNXECl.etisw2_servlet_user?qnaid=QNAS20200728000002&pgno=1']\n"
     ]
    }
   ],
   "source": [
    "url_head = \"https://www.sisul.or.kr\" # url 가져오기  \n",
    "\n",
    "url_board = url_head + \"/open_content/childrenpark/qna/qnaMsgList.do?pgno=1\"\n",
    "# url 가져오기  + \"/open_content/childrenpark/qna/qnaMsgList.do?pgno=1\" 추가 \n",
    "\n",
    "res = request.urlopen(url_board)  # url을 요청해서 텍스트화해서 저장 \n",
    "soup = BeautifulSoup(res, \"html.parser\") # html 가져오기 \n",
    "\n",
    "# selector = \"#detail_con > div.generalboard > table > tbody > tr > td.left.title > a\"\n",
    "selector = \"#detail_con > div.generalboard > table > tbody > tr > td.left.title > a\"\n",
    "titles = [] # 빈 리스트를 만듬\n",
    "links = []\n",
    "for a in soup.select(selector):\n",
    "    titles.append(a.text) # text가 나오면 titles 리스트에 추가 \n",
    "    links.append(url_head + a.attrs[\"href\"]) # links 리스트에 추가 \n",
    "    \n",
    "print(titles, links) # titles, links 출력 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>관리인 마스크</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어린이 대공원 쓰레기집하장 내 쓰레기 제거 요청</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>마스크미착용으로 축구 및, 베트민턴 치는 인원이 너무 많아요.</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>공원 내 마스크 착용</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>청춘핫도그 점장님과 직원분께 감사드립니다</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title  \\\n",
       "0                             관리인 마스크   \n",
       "1         어린이 대공원 쓰레기집하장 내 쓰레기 제거 요청    \n",
       "2  마스크미착용으로 축구 및, 베트민턴 치는 인원이 너무 많아요.   \n",
       "3                         공원 내 마스크 착용   \n",
       "4              청춘핫도그 점장님과 직원분께 감사드립니다   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "1  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "2  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "3  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "4  https://www.sisul.or.kr/open_content/childrenp...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # 판다스를 불러옴\n",
    "\n",
    "\n",
    "board_df = pd.DataFrame({\"title\": titles, \"link\": links}) # 데이터 프레임을 만들어 저장 \n",
    "board_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_df.to_csv(\"board.csv\", index=False) # board.csv라는 이름의 csv파일로 저장 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
