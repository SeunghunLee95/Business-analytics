{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 마이닝이란? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 비정형 데이터를 정형데이터 즉, 일정한 길이의 vector로 변환하여 사용하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 텍스트를 머신러닝으로 다룰 수 있는 형태로 변환한 다음 변환 된 것에 머신러닝을 적용시켜 원하는 것을 하는것 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝 단계 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 문서 -> tokenize/normalize -> 표준화된 단어들 시퀀스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 마이닝 도구 - 파이썬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ NTLK - 가장 많이 알려진 NLP 라이브러리\n",
    "+ Scikit Learn - 머신러닝 라이브러리 / 기본적인 NLP, 텍스트 마이닝 관련 도구 지원\n",
    "+ Gensim - Word2Vec / 텍스트 마이닝 관련 도구 지원 \n",
    "+ Keras - RNN, seq2seq 등 딥러닝 위주의 라이브러리 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 마이닝 기본도구 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 문서나 문장 등을 sparse vector(숫자로 표현된 0을 많이 포함한 벡터)로 변환\n",
    "+ Tokenize - 대상이 되는 문서/문장을 최소 단위로 쪼갬, 한글은 구조상 형태소 분석 필요  \n",
    "+ Text normalization - 최소 단위를 표준화, Stemming(어간 추출), Lemmantization(표제어 추출) \n",
    "+ POS-tagging - 최소 의미 단위로 나누어진 대상에 품사 부착, 품사를 알기 위해서 문맥을 파악해야함  \n",
    "+ Chunking - pos-tagging결과를  말모듬으로 다시 합치는 과정\n",
    "+ 개체명 인식(NER) - 텍스트로부터 의미 있는 정보를 추출, 명사구/ 관계 인식 \n",
    "+ BOW, TFIDF - tokenized 결과를 이용하여 문서를 vector로 표현 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification with BOW/TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Naive Bayes\n",
    "+ Logistic Regression\n",
    "+ Ridge and Lasso Regression - 릿지 회귀/ 라쏘 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문서분류의 활용 - Sentiment Analysis(감성분석)\n",
    "+ 문서의 긍정/부정 판단 \n",
    "+ 예시) label이 0이면 부정, 1이면 긍정 -> 리뷰를 BOW로 변환 후 input으로 쓰고, label을 target으로 하여 학습 -> 나이브베이즈, 로지스틱회귀분석, SVM 등 다양한 방법 사용 -> 새로운 리뷰에 대해 긍정, 부정 예측  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝의 문제 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 차원의 저주  \n",
    "+ 데이터 간의 거리가 너무 멀게 위치 \n",
    "+ 해결방법 - 더많은 데이터/ 차원축소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 빈도의 불균형 \n",
    "+ 멱법칙 - 극히 소수의 데이터가 결정적인 영향 \n",
    "+ 해결 방안 - 빈도 높은 단어를 삭제/ 유무 변환 / log 등의 함수 이용해 weight 변경 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어가 쓰인 순서정보의 손실 \n",
    "+ 통계에 대한 의미 파악 vs 순서에 의한 의미 파악 -> Bow 문맥정보 상실(통계에 대한 의미파악)\n",
    "+ 단어들의 순서 손실 \n",
    "+ 해결방안 - n-gram(부분적 해결, classification에서 유용) / 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제의 해결방안"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 차원 축소 \n",
    "+ Feature selection - 일정 갯수를 고르면 나머지는 버린다\n",
    "+ Feature extraction - 전체에서 정보를 추출하여 일정 갯수를 고른다 \n",
    "+ Embedding \n",
    "+ Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction \n",
    "+ PCA - 데이터의 분산을 최대한 보존하는 새로운 축을 찾아 변환해서 차원 축소 \n",
    "+ 주성분 분석 = 고유값이 큰 순서대로 고유벡터를 정렬하여 차원 선택 후 고유벡터와 선형결합으로 차원 축소 \n",
    "\n",
    "+ LSA - SVD \n",
    "+ 특이값 분해 - Truncated SVD  \n",
    "\n",
    "+ 잠재의미 분석 - 문서 간의 유사도, 단어 간의 유사도  \n",
    "\n",
    "+ Topic Modeling  - 문서가 많을 때 분석하는 방법 \n",
    "+ Latent Dirichlet Allocation\n",
    "+ Topic Trend "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embedding \n",
    "+ 단어에 대한 vector의 dimension reduction이 목표\n",
    "+ 단어의 표현 - Term-Document Matrix에서 Document별 count vector(일반화가 어려움)/ one-hot-encoding : 각 단어를 모든 문서에서 사용된 단어들의 수 길이의 벡터로 표현  \n",
    "+ Word embedding - one-hot-encoding으로 표현된 단어를 dense vector로 변환 -> 변환된 vector를 이용하여 학습 -> 최종 목적에 맞게 학습에 의해 vector가 결정 -> 학습목적 관점에서의 단어 의미 내포 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bow와 Word Embedding의 차이\n",
    "+ 순서의 유무 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding을 이용한 문서 분류 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "+ 문장에 나타난 단어들의 순서를 이용해 word embedding을 수행 \n",
    "+ CBOW : 주변 단어들을 이용해 다음 단어 예측 \n",
    "+ Skip-gram : 한 단어의 주변 단어들을 예측 \n",
    "\n",
    "#### Word2Vec의 학습방법 \n",
    "+ sliding window를 이용한 학습set구성 : 주어진 주변 단어들을 입력했을 때, target word의 확률이 높아지도록 학습 또는 그 반대  \n",
    "+ embedding vector : input이 one-hot vector이므로 W가 embedding vector의 집합이됨 \n",
    "\n",
    "#### Word2Vec의 의미 \n",
    "+ 단어의 위치에 기반하여 의미를 내포하는 vector 생성 : 비슷한 위치에 나타나는 단어들은 비슷한 vector를 가지게 됨 -> 단어간의 유사성을 이용하여 연산 가능\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELMo\n",
    "+ 사전 훈련된 언어 모델을 사용하는 Word embedding 방법론 \n",
    "+ 문맥을 반형하기 위해 개발 \n",
    "+ 문맥 파악을 위해 biLSTM으로 학습된 모형 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer Learing \n",
    "+ 텍스트 마이닝에서의 전이 학습  \n",
    "+ feature level : 단어에 대한 dense vector를 새로 학습하지 않고 학습된 vector를 그래도 씀 \n",
    "+ model level : word embedding과 모형 전체를 가져다 학습 \n",
    "+ Word2Vec, Glove, ELMo 등의 사전학습된 word embedding이 전이학습에 많이 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Embedding \n",
    "+ fixed dense embedding \n",
    "+ Word2Vec 모형에서 주변단어들에 더하여 document의 고유한 vector를 함께 학습함으로써 document에 대한 dense vector 생성 \n",
    "+ dense vector를 이용해 매칭,분류 등의 작업 수행 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBM \n",
    "+ 사전학습 목적 - 차원을 변경하면서 원래의 정보량 최대한 유지가 목적 \n",
    "+ 사전학습을 통한 차원 축소에 사용 가능 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder\n",
    "+ RBM과 유사한 개념 : encoder로 차원을 축소하고 decoder로 다시 복원 했을 때 원래의 X와 최대한 동일하도록 학습 \n",
    "+ 작동방식은 PCA와 유사 : 데이터에 내재된 일정한 구조 - 연관성을 추출 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context(sequence)의 파악 \n",
    "#### N-gram\n",
    "+ 문맥 파악을 위한 전통적인 방법 \n",
    "+ Unigram, Bi-gram(2개 단위 기준), Tri-gram(3개 단위)\n",
    "+ 보통 Unigram에  Bi-gram, Tri-gram을 추가하면서 feature의 수를 증가시켜 사용\n",
    "+ 장점 : 문맥 파악에 유리, 단점: dimension 증가 \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝과 텍스트마이닝 \n",
    "#### RNN \n",
    "+ 자신뿐 아니라 앞에 어떤 단어가 왔느냐에 따라 히든 node결정 즉, 문맥적 정보 축적 \n",
    "+ RNN의 문제 : 문장의 길이가 길수록 층이 깊은 형태 -> 경사 소실 -> 앞부분의 단어 정보 학습 안됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM\n",
    "+ 직통 경로를 만들어 RNN 문제 해결\n",
    "+ LSTM 문제 : 단어 순서가 갖는 문맥 정보가 한 방향으로만 학습, 자신의 뒤에 오는 단어에 의해 영향을 받는 경우 학습이 안됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-LSTM\n",
    "+ 양방향으로 LSTM을 구성하여 두 결과를 합침 \n",
    "+ 양방향 순서 모두 학습 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 합성곱 신경망(CNN) \n",
    "+ 이미지 처리를 위해 개발된 신경망 \n",
    "+ 주변 정보를 학습한다는 점을 이용하여 텍스트의 문맥 학습 \n",
    "+ 합성곱층과 풀링층으로 구성 \n",
    "\n",
    "#### CNN 문서분류 \n",
    "+ 단어 시퀀스에 대해 CNN 필터는 1차원으로만 적용되고 텍스트의 특징을 추출한 결과를 분류기에 넣어서 문서를 판별 \n",
    "\n",
    "#### Sequence-to-sequence \n",
    "+ 입력은 sequence, 출력은 하나의 값인 경우가 일반적, 번역, chat-bot,summarize 등은 출력도 sequence\n",
    "+ encoder,decoder의 구조를 가짐 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention \n",
    "+ 출력에 나온 어떤 단어는 입력에 있는 특정 단어들에 민감 \n",
    "+ 입력의 단어들로부터 출력 단어에 직접 링크를 만듦 \n",
    "\n",
    "#### Transformer\n",
    "+ 입력 단어들끼리도 상호연관성이 있는 것에 착안 \n",
    "+ 입력 -> 출력으로의 attention 외에 입력 단어들 간의 attention, 입력 + 출력 -> 출력으로서의 attention 추가 \n",
    "+ encoder와 decoder가 서로 다른 attention 구조를 사용 \n",
    "+ RNN이 사라지고 self-attention이 이를 대신 \n",
    "\n",
    "#### BERT\n",
    "+ 양방향 transformer 인코더를 사용 - transformer에 기반한 OpenAI GPT와의 차이(decoder 만을 씀) \n",
    "+ transfer learning에서 feature + model을 함께 transfer하고 fine tuning을 통해 적용하는 방식 - 전이학습과 결합 \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
